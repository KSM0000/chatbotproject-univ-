import requests
from bs4 import BeautifulSoup
from datetime import datetime
import psycopg2
import logging
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
import chromadb
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
from openai import OpenAI
import gradio as gr
import re

# -------------------- OpenAI ì„¤ì • --------------------
client = OpenAI(api_key="")
embedding_function = OpenAIEmbeddingFunction(api_key=client.api_key, model_name="text-embedding-3-small")

# âœ… ChromaDB Persistent ì„¤ì • (ì‹ ê·œ ë°©ì‹)
chroma_client = chromadb.PersistentClient(path="./chroma_db")

collection = chroma_client.get_or_create_collection(
    name="cu-notices",
    embedding_function=embedding_function
)

# -------------------- êµìˆ˜ëª… ì¶”ì¶œ --------------------
def extract_professor_name(user_query):
    cleaned = re.sub(r"(êµìˆ˜ë‹˜|êµìˆ˜|ê´€ë ¨|ì •ë³´|ì•Œë ¤ì¤˜|ì•Œë ¤ì£¼ì„¸ìš”|ì°¾ì•„ì¤˜|ì— ëŒ€í•´ ì•Œë ¤ì¤˜|ì— ê´€í•´ ì•Œë ¤ì¤˜|ì— ê´€í•´ì„œ ì•Œë ¤ì¤˜)", "", user_query)
    return cleaned.strip()

# -------------------- GPT ì§ˆë¬¸ ì²˜ë¦¬ --------------------
def simplify_query(query):
    stop_phrases = ["ê´€ë ¨", "ê³µì§€", "ì •ë³´", "ì•Œë ¤ì¤˜", "ì•Œë ¤ì£¼ì„¸ìš”", "ì— ëŒ€í•´", "ì— ê´€í•´", "ì— ê´€í•œ", "ì°¾ì•„ì¤˜", "ê²€ìƒ‰"]
    for phrase in stop_phrases:
        query = query.replace(phrase, "")
    return query.strip()

def generate_gpt_reply(user_query):
    simplified_query = simplify_query(user_query)

    try:
        results = collection.query(query_texts=[simplified_query], n_results=20)
        docs = results.get("documents", [[]])[0]
        metas = results.get("metadatas", [[]])[0]
    except Exception as e:
        logging.error(f"ChromaDB ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
        return "âŒ ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

    if not docs:
        return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    core_keywords = [
        "ëŒ€êµ¬ê°€í†¨ë¦­ëŒ€í•™êµ", "DCU",
        "ì¥í•™ê¸ˆ", "ë“±ë¡ê¸ˆ", "íœ´í•™", "ì¡¸ì—…", "ê°•ì˜ì‹¤", "ë„ì„œê´€", "ì‹œí—˜",
        "ìŠ¤ì¿¨ë²„ìŠ¤", "ì…”í‹€ë²„ìŠ¤", "í†µí•™ë²„ìŠ¤", "ë²„ìŠ¤ ìš´í–‰", "ë²„ìŠ¤ ì•ˆë‚´",
        "ì±„ìš©", "ê°•ì˜", "ì…í•™", "ì„±ì ", "ë©´ì ‘", "ì·¨ì—…"
    ]

    professor_keywords = ["êµìˆ˜", "êµìˆ˜ë‹˜"]

    normal_results = []
    professor_results = []

    for doc, meta in zip(docs, metas):
        source = meta.get("source", "")

        # êµìˆ˜ ì •ë³´ ë”°ë¡œ ì €ì¥
        if source == "professor":
            professor_results.append(doc)
            continue

        # ì¼ë°˜ ê³µì§€ í•„í„°ë§
        if any(k in doc.lower() for k in core_keywords):
            link = meta.get("link")
            normal_results.append(f"{doc}\n(ë§í¬: {link})" if link else doc)

    # ì·¨ì—…ì¼ ê²½ìš° ì™¸ë¶€ ì•ˆë‚´ í¬í•¨
    if "ì·¨ì—…" in user_query:
        external_info = (
            "ë˜í•œ, ì™¸ë¶€ ì·¨ì—…ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì‚¬ì´íŠ¸ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
            "- ì‚¬ëŒì¸: https://www.saramin.co.kr\n"
            "- ì¡ì½”ë¦¬ì•„: https://www.jobkorea.co.kr"
        )
        if not normal_results:
            return f"ë‚´ë¶€ ê³µì§€ì‚¬í•­ì—ì„œëŠ” ê´€ë ¨ëœ ì·¨ì—… ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n\n{external_info}"
        else:
            return f"ğŸ“¢ ë‚´ë¶€ ê³µì§€ì‚¬í•­ì˜ ì·¨ì—… ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n\n" + "\n\n".join(normal_results) + "\n\n" + external_info

    # êµìˆ˜ ì§ˆë¬¸ì¼ ë•Œë§Œ êµìˆ˜ì •ë³´ ì¶œë ¥
    if any(k in user_query for k in professor_keywords):
        if professor_results:
            return "\n\n".join(professor_results)
        else:
            return "í•´ë‹¹ êµìˆ˜ë‹˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ì¼ë°˜ ì§ˆë¬¸ ê²°ê³¼
    if normal_results:
        context = "\n".join(normal_results)
        prompt = (
            f"í•™ìƒì´ '{user_query}' ë¼ê³  ë¬¼ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” ëŒ€êµ¬ê°€í†¨ë¦­ëŒ€í•™êµ(DCU) ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n\n"
            f"{context}\n\n"
            f"ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì—ê²Œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜."
        )

        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ë„ˆëŠ” ëŒ€í•™êµ ì •ë³´ë¥¼ ì•ˆë‚´í•˜ëŠ” ì±—ë´‡ì´ì•¼. ì§ˆë¬¸ì— ëŒ€í•´ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´."},
                    {"role": "user", "content": prompt}
                ]
            )
            return response.choices[0].message.content

        except Exception as e:
            logging.error(f"OpenAI ì‘ë‹µ ì˜¤ë¥˜: {e}")
            return "GPT ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."


# -------------------- LOG ì„¤ì • --------------------
logging.basicConfig(
    filename="crawler.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# -------------------- PostgreSQL ì €ì¥ + ë²¡í„° ì €ì¥ --------------------
def save_to_postgres(data):
    conn = psycopg2.connect(
        dbname="",
        user="",
        password="",
        host="",
        port=
    )
    cur = conn.cursor()
    new_count = 0

    for row in data:
        try:
            cur.execute("""
                INSERT INTO notices (timestamp, source, title, link)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (link) DO NOTHING
            """, row)
            new_count += 1

            title, link, source = row[2], row[3], row[1]
            doc_id = link
            content = f"[{source}] {title}"
            collection.add(
                documents=[content],
                metadatas=[{"source": source, "link": link}],
                ids=[doc_id]
            )

        except Exception as e:
            logging.error(f"INSERT ì‹¤íŒ¨: {e}")
            continue

    conn.commit()

    try:
        cur.execute("SELECT name, office, phone, email, expertise FROM professors")
        rows = cur.fetchall()
        for row in rows:
            name, office, phone, email, expertise = row
            content = f"{name} êµìˆ˜ë‹˜ | ì—°êµ¬ì‹¤: {office} | ì—°ë½ì²˜: {phone} | ì´ë©”ì¼: {email} | ì „ê³µ: {expertise}"
            collection.add(
                documents=[content],
                metadatas=[{"source": "professor", "name": name}],
                ids=[f"prof-{name}"]
            )
    except Exception as e:
        logging.error(f"êµìˆ˜ ì •ë³´ ì¶”ê°€ ì‹¤íŒ¨: {e}")

    conn.close()
    logging.info(f"{new_count}ê±´ ì €ì¥ ì™„ë£Œ (ì¤‘ë³µ ì œì™¸)")
    print(f"âœ… PostgreSQLì— {new_count}ê±´ ì €ì¥ ì™„ë£Œ")

# -------------------- ì…í•™ì •ë³´ì²˜ í¬ë¡¤ëŸ¬ --------------------
def crawl_admission_notices():
    url = "https://ibsi.cu.ac.kr/kor/bbs/BBSMSTR_000000000090/lst.do"
    base_url = "https://ibsi.cu.ac.kr"
    res = requests.get(url)
    soup = BeautifulSoup(res.text, "html.parser")
    rows = soup.select("div.board.notice.list > ul > li")[1:]

    result = []
    for row in rows:
        title_tag = row.select_one("div.ntt_sj a")
        if title_tag:
            title = title_tag.get_text(strip=True)
            onclick_attr = title_tag.get("onclick", "")
            if "bbsMstrView(" in onclick_attr:
                notice_id = onclick_attr.split("bbsMstrView('")[1].split("'")[0]
                link = f"{base_url}/kor/BBSMSTR_000000000090/{notice_id}/view.do"
                result.append([datetime.now().isoformat(), "ì…í•™ì •ë³´ì²˜", title, link])
    return result

# -------------------- í•™êµì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ --------------------
def crawl_school_notices():
    base_url = "https://www.cu.ac.kr"
    target_url = base_url + "/plaza/notice/notice"
    result = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(target_url)

        try:
            page.wait_for_selector("table", timeout=15000)
        except PlaywrightTimeout:
            print("âŒ table ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤ (íƒ€ì„ì•„ì›ƒ)")
            page.screenshot(path="school_timeout.png")
            browser.close()
            return []

        soup = BeautifulSoup(page.content(), "html.parser")
        rows = soup.select("table > tbody > tr")

        for row in rows:
            title_tag = row.select_one("td:nth-child(2) a")
            if title_tag:
                title = title_tag.get_text(strip=True)
                href = title_tag.get("href", "")
                if href.startswith("/"):
                    link = base_url + href
                    result.append([datetime.now().isoformat(), "í•™êµì‚¬ì´íŠ¸", title, link])

        browser.close()

    return result

# -------------------- ì‹¤í–‰ ë£¨í‹´ --------------------
def run_crawlers():
    logging.info("í¬ë¡¤ë§ ì‹œì‘")
    print(f"â° ì‹¤í–‰: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    all_data = []
    try:
        admission = crawl_admission_notices()
        school = crawl_school_notices()

        print("ğŸ“˜ ì…í•™ì •ë³´ì²˜:", len(admission))
        print("ğŸ« í•™êµì‚¬ì´íŠ¸:", len(school))

        if admission: print("ì…í•™ì •ë³´ì²˜ ì˜ˆì‹œ â†’", admission[0])
        if school: print("í•™êµì‚¬ì´íŠ¸ ì˜ˆì‹œ â†’", school[0])

        all_data.extend(admission)
        all_data.extend(school)

    except Exception as e:
        logging.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        print("âŒ ì˜¤ë¥˜ ë°œìƒ:", e)
        return

    save_to_postgres(all_data)

# -------------------- Gradio UI --------------------
with gr.Blocks(theme=gr.themes.Soft(), title="DCU ì±—ë´‡") as demo:
    with gr.Column():
        gr.Markdown("""
        ## ğŸ“ DCU ì±—ë´‡
        ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ìµœê·¼ ê³µì§€ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ ì•ˆë‚´í•´ë“œë¦´ê²Œìš”.  
        ì˜ˆ) `ì¥í•™ê¸ˆ`, `ì¡¸ì—…`, `ì±„ìš©`, `íœ´í•™`, `ooo êµìˆ˜ë‹˜ ì •ë³´`
        """)

    chatbot = gr.Chatbot(label="DCU ì±—ë´‡", type='messages')
    input_text = gr.Textbox(label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ì¥í•™ê¸ˆ ì¼ì •, ì±„ìš© ê³µì§€ ë“±")
    submit_btn = gr.Button("ì§ˆë¬¸í•˜ê¸°")
    state = gr.State([])

    def chatbot_reply(msg, history):
        full_reply = generate_gpt_reply(msg)
        history.append({"role": "user", "content": msg})
        reply = ""
        for char in full_reply:
            reply += char
        yield history + [{"role": "assistant", "content": reply}], history + [{"role": "assistant", "content": reply}]

    submit_btn.click(
        fn=chatbot_reply,
        inputs=[input_text, state],
        outputs=[chatbot, state]
    )

# -------------------- ë©”ì¸ ì‹¤í–‰ --------------------
if __name__ == "__main__":
    run_crawlers()
    demo.launch(server_name="0.0.0.0", server_port=8000)